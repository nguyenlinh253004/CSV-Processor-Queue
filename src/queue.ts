import { Queue, Worker } from "bullmq";
import { Worker as WorkerThread } from "worker_threads";
import IORedis from "ioredis";
import fs from "fs";
import path from "path";
import { AppDataSource } from "./data-source";
import { User } from "./entity/User";
import { userCache } from "./cache";
import logger from "./logger"; // Import Winston logger
import { jobEvents } from "./socketEvents";
const connection = new IORedis({
  host: "localhost",
  port: 6379,
  maxRetriesPerRequest: null,
});

export const csvQueue = new Queue("csv-processing", { connection });

const worker = new Worker(
  "csv-processing",
  async (job) => {
    const { filePath, originalName } = job.data;
    logger.info(`ðŸš€ Báº¯t Ä‘áº§u Job ${job.id} - File: ${originalName}`);

    // Æ¯á»›c tÃ­nh sá»‘ dÃ²ng Ä‘á»ƒ lÃ m Progress
    const fileStats = fs.statSync(filePath);
    const estimatedTotal = Math.max(1, Math.round(fileStats.size / 60)); // Giáº£ Ä‘á»‹nh ~60 bytes/dÃ²ng

    if (!AppDataSource.isInitialized) await AppDataSource.initialize();
    const userRepository = AppDataSource.getRepository(User);

    try {
      // BÆ¯á»šC 1: PARSE (Worker Thread)
      const parsedUsers = await new Promise<Partial<User>[]>((resolve, reject) => {
        const workerThread = new WorkerThread(path.resolve(__dirname, "./workers/csv-parser-worker.js"), {
          workerData: { chunkFilePath: filePath },
        });
        workerThread.on("message", (res) => res.status === "success" ? resolve(res.users) : reject(new Error(res.error)));
        workerThread.on("error", reject);
      });

      // BÆ¯á»šC 2: INSERT BATCH & PROGRESS
      let processedCount = 0;
      const batchSize = 500;
      let batch: Partial<User>[] = [];

      for (const user of parsedUsers) {
        batch.push(user);
        if (batch.length >= batchSize) {
          await userRepository
          .createQueryBuilder()
          .insert()
          .into(User)
          .values(batch)
          .orIgnore(true)  // Ignore náº¿u vi pháº¡m unique (email trÃ¹ng)
          .execute();
          processedCount += batch.length;
          
          // Cáº­p nháº­t Progress cho BullMQ
          const progress = Math.min(100, Math.round((processedCount / estimatedTotal) * 100));
          await job.updateProgress(progress);
          // Trong worker, khi update progress:
          jobEvents.emit("job-progress", { jobId: job.id, progress: progress });
          logger.info(`Job ${job.id} Progress: ${progress}%`);
          batch = [];
        }
      }

      if (batch.length > 0) {
        await userRepository
          .createQueryBuilder()
          .insert()
          .into(User)
          .values(batch)
          .orIgnore(true)  // Ignore náº¿u vi pháº¡m unique (email trÃ¹ng)
          .execute();
        processedCount += batch.length;
        await job.updateProgress(100);
      }

      userCache.flushAll();
      logger.info("Cache flushed sau khi insert má»›i");
      // XÃ³a file async, chá»‰ khi thÃ nh cÃ´ng
      if (fs.existsSync(filePath)) fs.unlinkSync(filePath);
      logger.info(`ðŸ Job ${job.id} hoÃ n thÃ nh thÃ nh cÃ´ng!`);
      return { status: "success", total: processedCount };

    } catch (error) {
      logger.error(` Lá»—i táº¡i Job ${job.id}:`, error);
      throw error; // QuÄƒng lá»—i Ä‘á»ƒ BullMQ kÃ­ch hoáº¡t Retry
    }
  },
  {
    connection,
    concurrency: 1,
    // Cáº¤U HÃŒNH RETRY Táº I ÄÃ‚Y
  }
);
worker.on("completed", (job) => {
  logger.info(`Job ${job.id} completed:`, job.returnvalue);
});

worker.on("failed", (job, err) => {
  logger.error(`Job ${job?.id} failed sau ${job?.attemptsMade || 0} attempts:`, err);
});
console.log(" BullMQ Queue & Worker (vá»›i CSV processing) Ä‘Ã£ khá»Ÿi táº¡o");